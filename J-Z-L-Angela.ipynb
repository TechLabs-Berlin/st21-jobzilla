{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "supported-metabolism",
   "metadata": {},
   "source": [
    "## Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "first-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import textract\n",
    "from spacy.pipeline import EntityRuler\n",
    "from spacy import displacy\n",
    "import jsonlines\n",
    "from spacy.lang.en import English\n",
    "from spacy.tokens import Doc\n",
    "import spacy\n",
    "import PyPDF2\n",
    "import json, requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-margin",
   "metadata": {},
   "source": [
    "## 1. Upload resume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "international-visit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in /home/pandmi/anaconda3/lib/python3.8/site-packages (1.26.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "wireless-thomson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "file = '/content/Pankov_D_CV.pdf'\n",
    "\n",
    "def extract_text_from_pdf(file):\n",
    "   \n",
    "    fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "    page_count = fileReader.getNumPages()\n",
    "    text = [fileReader.getPage(i).extractText() for i in range(page_count)]\n",
    "    \n",
    "    return str(text).replace(\"\\\\n\", \"\")\n",
    "\n",
    "def extract_candidate_skills(file):\n",
    "    candidate_skills = get_skills(extract_text_from_pdf(file))  \n",
    "        \n",
    "    return candidate_skills\n",
    "\n",
    "cand_skills = extract_candidate_skills(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "exterior-fairy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "grave-cattle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sudo jupyter nbextension install --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "moderate-spine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7213d6319e784be18ceb9e5d26b6619a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value={}, accept='.txt', description='Upload', multiple=True)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from ipywidgets import FileUpload\n",
    "# from IPython.display import display\n",
    "# upload = FileUpload(accept='.txt', multiple=True)\n",
    "# display(upload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# File Extension. set as 'pdf' or as 'doc(x)'\n",
    "extension = 'pdf'\n",
    "\n",
    "def create_tokenized_texts_list(extension):\n",
    "    '''Create two lists, one with the names of the candidate and one with the tokenized \n",
    "       resume texts extracted from either a .pdf or .doc'''\n",
    "    resume_texts, resume_names = [], []\n",
    "    \n",
    "    # Loop over the contents of the directory containing the resumes, filtering by .pdf or .doc(x)\n",
    "    for resume in list(filter(lambda x: extension in x, os.listdir(PROJECT_DIR + '/CV'))):\n",
    "        if extension == 'pdf':\n",
    "            # Read in every resume with pdf extension in the directory\n",
    "            resume_texts.append(nlp(extract_text_from_pdf(PROJECT_DIR + '/CV/' + resume)))\n",
    "        elif 'doc' in extension:\n",
    "            # Read in every resume with .doc or .docx extension in the directory\n",
    "            resume_texts.append(nlp(extract_text_from_word(PROJECT_DIR + '/CV/' + resume)))\n",
    "            \n",
    "        resume_names.append(resume.split('_')[0].capitalize())\n",
    "        \n",
    "    return resume_texts, resume_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nutritional-upper",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "skill_pattern_path = \"jz_skill_patterns.jsonl\"\n",
    "with jsonlines.open(\"jz_skill_patterns.jsonl\") as f:\n",
    "    created_entities = [line['label'].upper() for line in f.iter()]\n",
    "\n",
    "ruler = EntityRuler(nlp).from_disk(skill_pattern_path)\n",
    "\n",
    "nlp.add_pipe(ruler, after='parser')\n",
    "\n",
    "def get_skills(text):\n",
    "    doc = nlp(text)\n",
    "    myset = []\n",
    "    subset = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_==\"SKILL\":\n",
    "            subset.append(ent.text)\n",
    "    myset.append(subset)\n",
    "    return subset\n",
    "\n",
    "\n",
    "def extract_text_from_pdf(file):\n",
    "    fileReader = PyPDF2.PdfFileReader(open(file,'rb'))\n",
    "    page_count = fileReader.getNumPages()\n",
    "    text = [fileReader.getPage(i).extractText() for i in range(page_count)]\n",
    "    \n",
    "    return str(text).replace(\"\\\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "biblical-teens",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'CV.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "unusual-client",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C',\n",
       " 'support',\n",
       " 'analytics',\n",
       " 'Marketing',\n",
       " 'Google',\n",
       " 'monitoring',\n",
       " 'Play',\n",
       " 'Marketing',\n",
       " 'analytics',\n",
       " 'Data analysis',\n",
       " 'Marketing',\n",
       " 'Marketing',\n",
       " 'LANGUAGES',\n",
       " 'SOFTWARE',\n",
       " 'analytics',\n",
       " 'Jupyter',\n",
       " 'Superset',\n",
       " 'Marketing',\n",
       " 'Google',\n",
       " 'Business',\n",
       " 'Marketing',\n",
       " 'Google Analytics',\n",
       " 'Google Tag Manager',\n",
       " 'Analytics',\n",
       " 'Design',\n",
       " 'Python',\n",
       " 'Pandas',\n",
       " 'Numpy',\n",
       " 'Java',\n",
       " 'SQL',\n",
       " 'HTML',\n",
       " 'CSS',\n",
       " 'Git',\n",
       " 'Linux',\n",
       " 'Marketing',\n",
       " 'Google Analytics']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_skills = get_skills(extract_text_from_pdf(file))\n",
    "cv_skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-march",
   "metadata": {},
   "source": [
    "## 2. Search for a job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indian-intervention",
   "metadata": {},
   "source": [
    "### Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cordless-bosnia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install praw timeago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "equipped-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import praw\n",
    "import timeago\n",
    "\n",
    "\n",
    "class Job():\n",
    "    def __init__(self,post):\n",
    "        self.title = post.title\n",
    "        self.date = compute_ago(post.created_utc)\n",
    "        self.url = post.url\n",
    "\n",
    "\n",
    "def compute_ago(created):\n",
    "    date = datetime.datetime.fromtimestamp(int(float(str(created))))\n",
    "    now = datetime.datetime.now()\n",
    "    print (timeago.format(date, now))\n",
    "    return timeago.format(date, now)\n",
    "\n",
    "\n",
    "client = \"xxxx\" # Client ID\n",
    "secret = \"xxxxxxxxxx\" # Secret key\n",
    "\n",
    "# Function to connect to Reddit API\n",
    "def connect(cl_id, cl_secret):\n",
    "    return praw.Reddit(client_id = cl_id, client_secret = cl_secret, user_agent = \"Jobzilla\")\n",
    "\n",
    "def search_reddit(query):\n",
    "    red = connect(client, secret)\n",
    "    # Generating the string for keywords\n",
    "    keys = query.split(\",\")\n",
    "    subs = [\"jobbit\", \"forhire\"]\n",
    "\n",
    "    # Generate string for keys\n",
    "    str_keys = \"(title : \\\"\" +keys[0]+ \"\\\"\"\n",
    "    for k in keys[1:]:\n",
    "        str_keys = str_keys + \" OR title:\\\"\" + k + \"\\\"\"\n",
    "    str_keys = str_keys + \")\"\n",
    "\n",
    "    # Generate string for subs\n",
    "    str_subs = \"(subreddit:hiring\"\n",
    "    for sub in subs:\n",
    "        str_subs = str_subs + \" OR subreddit:\" + sub\n",
    "    str_subs = str_subs + \") \"\n",
    "\n",
    "    # Creating the full search query using Reddit search syntax\n",
    "    full_str = \"(title:\\\"hiring\\\" OR flair:Hiring) AND  \" + str_keys + \" AND \" + str_subs\n",
    "\n",
    "    # making the call\n",
    "    all = red.subreddit(\"all\")\n",
    "    \"\"\"\n",
    "    for post in all.search(query = full_str, sort = \"new\"):\n",
    "        print post.title\n",
    "        post_date = datetime.datetime.utcfromtimestamp(post.created_utc)\n",
    "        print str(post_date.month) + \", \" + str(post_date.day) + \" in \" + str(post_date.year)\n",
    "    \"\"\"\n",
    "    return all.search(query = full_str, sort = \"new\")\n",
    "    # return all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "necessary-deputy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<praw.models.listing.generator.ListingGenerator at 0x7f35824ba700>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword = 'python, sql'\n",
    "\n",
    "all = search_reddit(query = keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assisted-auckland",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = { \"title\":[],\n",
    "                \"subreddit\":[],\n",
    "                \"score\":[], \n",
    "                \"id\":[], \n",
    "                \"url\":[], \n",
    "                \"comms_num\": [], \n",
    "                \"created\": [], \n",
    "                \"body\":[]}\n",
    "\n",
    "for subreddit in all:\n",
    "    for submission in reddit.subreddit(subreddit):               \n",
    "        dict[\"title\"].append(submission.title)\n",
    "        dict['subreddit'].append(submission.subreddit)\n",
    "        dict[\"score\"].append(submission.score)\n",
    "        dict[\"id\"].append(submission.id)\n",
    "        dict[\"url\"].append(submission.url)\n",
    "        dict[\"comms_num\"].append(submission.num_comments)\n",
    "        dict[\"created\"].append(submission.created)\n",
    "        dict[\"body\"].append(submission.selftext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "critical-practice",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incomplete-warner",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('jobs_reddit_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abroad-importance",
   "metadata": {},
   "source": [
    "## 3. Create cover letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "wrong-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefixtext=\"<|startoftext|>~['Analytics','business','data science', 'business', 'algorithms']~Freelancer - Marketing Analyst\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "perfect-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "req = requests.post('https://jzl-api-v7otpcjevq-lz.a.run.app',\n",
    "                    json={'length': 500, 'temperature': 1.0, 'prefix':prefixtext})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = req.json()['text']\n",
    "print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
